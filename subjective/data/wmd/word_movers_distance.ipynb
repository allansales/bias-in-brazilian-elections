{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allan/anaconda2/lib/python2.7/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from pyemd import emd\n",
    "\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = \"raw_noticias_carta_2010-2018_folha_2018.csv\"\n",
    "path_processed_text_data = \"processed_noticias_carta_2010-2018_folha_2018.csv\"\n",
    "output = \"wmd_noticias_carta_2010-2018_folha_2018.csv\"\n",
    "initial_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicons Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_stopwords(text, stop_words_list):\n",
    "    list_words = text.split()\n",
    "    list_clean_text = []\n",
    "    for word in list_words:\n",
    "        if word not in stop_words_list:\n",
    "            list_clean_text.append(word)\n",
    "    return \" \".join(list_clean_text)\n",
    "\n",
    "def loadStopWordsPT(filename):\n",
    "    lines = [line.rstrip('\\n').strip() for line in open(filename)]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "argumentacao = \"a_ponto ao_menos apenas ate ate_mesmo incluindo inclusive mesmo nao_mais_que nem_mesmo no_minimo o_unico a_unica pelo_menos quando_menos quando_muito sequer so somente a_par_disso ademais afinal ainda alem alias como e e_nao em_suma enfim mas_tambem muito_menos nao_so nem ou_mesmo por_sinal tambem tampouco assim com_isso como_consequencia consequentemente de_modo_que deste_modo em_decorrencia entao logicamente logo nesse_sentido pois por_causa por_conseguinte por_essa_razao por_isso portanto sendo_assim ou ou_entao ou_mesmo nem como_se de_um_lado por_outro_lado mais_que menos_que nao_so tanto quanto tao como desde_que do_contrario em_lugar em_vez enquanto no_caso quando se se_acaso senao de_certa_forma desse_modo em_funcao enquanto isso_e ja_que na_medida_que nessa_direcao no_intuito no_mesmo_sentido ou_seja pois porque que uma_vez_que tanto_que visto_que ainda_que ao_contrario apesar_de contrariamente contudo embora entretanto fora_isso mas mesmo_que nao_obstante nao_fosse_isso no_entanto para_tanto pelo_contrario por_sua_vez porem posto_que todavia\"\n",
    "modalizacao = \"achar aconselhar acreditar aparente basico bastar certo claro conveniente crer dever dificil duvida efetivo esperar evidente exato facultativo falar fato fundamental imaginar importante indubitavel inegavel justo limitar logico natural necessario negar obrigatorio obvio parecer pensar poder possivel precisar predominar presumir procurar provavel puder real recomendar seguro supor talvez tem tendo ter tinha tive verdade decidir\"\n",
    "valoracao = \"absoluto algum alto amplo aproximado bastante bem bom categorico cerca completo comum consideravel constante definitivo demais elevado enorme escasso especial estrito eventual exagero excelente excessivo exclusivo expresso extremo feliz franco franqueza frequente generalizado geral grande imenso incrivel lamentavel leve maioria mais mal melhor menos mero minimo minoria muito normal ocasional otimo particular pena pequeno pesar pior pleno pobre pouco pouquissimo praticamente prazer preciso preferir principal quase raro razoavel relativo rico rigor sempre significativo simples tanto tao tipico total tremenda usual valer\"\n",
    "sentimento = \"abalar abater abominar aborrecer acalmar acovardar admirar adorar afligir agitar alarmar alegrar alucinar amar ambicionar amedrontar amolar animar apavorar apaziguar apoquentar aporrinhar apreciar aquietar arrepender assombrar assustar atazanar atemorizar aterrorizar aticar atordoar atormentar aturdir azucrinar chatear chocar cobicar comover confortar confundir consolar constranger contemplar contentar contrariar conturbar curtir debilitar decepcionar depreciar deprimir desapontar descontentar descontrolar desejar desencantar desencorajar desesperar desestimular desfrutar desgostar desiludir desinteressar deslumbrar desorientar desprezar detestar distrair emocionar empolgar enamorar encantar encorajar endividar enervar enfeiticar enfurecer enganar enraivecer entediar entreter entristecer entusiasmar envergonhar escandalizar espantar estimar estimular estranhar exaltar exasperar excitar execrar fascinar frustar gostar gozar grilar hostilizar idolatrar iludir importunar impressionar incomodar indignar inibir inquietar intimidar intrigar irar irritar lamentar lastimar louvar magoar maravilhar melindrar menosprezar odiar ofender pasmar perdoar preocupar prezar querer recalcar recear reconfortar rejeitar repelir reprimir repudiar respeitar reverenciar revoltar seduzir sensibilizar serenar simpatizar sossegar subestimar sublimar superestimar surpreender temer tolerar tranquilizar transtornar traumatizar venerar\" #malquerer obcecar\n",
    "pressuposicao = \"adivinhar admitir agora aguentar ainda antes atentar atual aturar comecar compreender conseguir constatar continuar corrigir deixar demonstrar descobrir desculpar desde desvendar detectar entender enxergar esclarecer escutar esquecer gabar ignorar iniciar interromper ja lembrar momento notar observar olhar ouvir parar perceber perder pressentir prever reconhecer recordar reparar retirar revelar saber sentir tolerar tratar ver verificar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_stop_words = loadStopWordsPT('./../process_text_expressions/stopwords.txt')\n",
    "\n",
    "argumentacao = clean_stopwords(argumentacao, raw_stop_words)\n",
    "modalizacao = clean_stopwords(modalizacao, raw_stop_words)\n",
    "valoracao = clean_stopwords(valoracao, raw_stop_words)\n",
    "sentimento = clean_stopwords(sentimento, raw_stop_words)\n",
    "pressuposicao = clean_stopwords(pressuposicao, raw_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SENTENCE_SIZE_THRESHOLD = 2\n",
    "\n",
    "def remove_dots(sentence):\n",
    "    sentence = re.sub('\\.*', '', sentence)\n",
    "    return sentence    \n",
    "\n",
    "def valid_sentence_size(sentence):\n",
    "    size = len(sentence.split())\n",
    "    if size >= SENTENCE_SIZE_THRESHOLD:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_valid_sentence(sentence):\n",
    "    sentence = remove_dots(sentence)\n",
    "    sentence = ' '.join(sentence.split())\n",
    "    return valid_sentence_size(sentence), sentence\n",
    "\n",
    "def text_rate(list_lex_rate):\n",
    "    if len(list_lex_rate) > 0:\n",
    "        return np.mean(list_lex_rate)\n",
    "    return -1\n",
    "\n",
    "def check_value(w):\n",
    "    if(w in vocab_dict):\n",
    "        return(vocab_dict[w])\n",
    "    return 0\n",
    "\n",
    "def lexicon_rate(lexicon, comment):\n",
    "    vect = CountVectorizer(token_pattern=pattern, strip_accents=None).fit([lexicon, comment])\n",
    "    v_1, v_2 = vect.transform([lexicon, comment])\n",
    "    v_1 = v_1.toarray().ravel()\n",
    "    v_2 = v_2.toarray().ravel()\n",
    "    W_ = W[[check_value(w) for w in vect.get_feature_names()]]\n",
    "    D_ = euclidean_distances(W_)\n",
    "    v_1 = v_1.astype(np.double)\n",
    "    v_2 = v_2.astype(np.double)\n",
    "    v_1 /= v_1.sum()\n",
    "    v_2 /= v_2.sum()\n",
    "    D_ = D_.astype(np.double)\n",
    "    D_ /= D_.max()\n",
    "    lex=emd(v_1, v_2, D_)\n",
    "    return(lex)\n",
    "\n",
    "def split_text_into_sentences(lexicon, text):\n",
    "    sent_text = nltk.sent_tokenize(text)\n",
    "    lex_rate = list()\n",
    "    for sentence in sent_text:\n",
    "        valid, sentence = is_valid_sentence(sentence)\n",
    "        if(valid):\n",
    "            lex_rate.append(lexicon_rate(lexicon, sentence))\n",
    "    return(text_rate(lex_rate))\n",
    "\n",
    "def wmd_ratings(text):\n",
    "    arg = split_text_into_sentences(argumentacao, text)\n",
    "    mod = split_text_into_sentences(modalizacao, text)\n",
    "    val = split_text_into_sentences(valoracao, text)\n",
    "    sen = split_text_into_sentences(sentimento, text)\n",
    "    pre = split_text_into_sentences(pressuposicao, text)\n",
    "    rates = list([text, arg, sen, val, mod, pre])\n",
    "    return(rates)\n",
    "\n",
    "def write_csv(lexicons_rates, all_info_text):\n",
    "    ratings_df = pd.DataFrame(lexicons_rates, columns=['text','arg','sen','val','mod','pre'])\n",
    "    df1_names = all_info_text.columns.values\n",
    "    df2_names = ratings_df.columns.values\n",
    "    colnames = np.concatenate([df1_names,df2_names], axis=0)\n",
    "    all_info = all_info_text.reset_index(drop=True, inplace=False)\n",
    "    ratings = ratings_df.reset_index(drop=True, inplace=False)\n",
    "    result_df = pd.concat([all_info, ratings], axis=1, ignore_index=True)\n",
    "    result_df.columns = colnames\n",
    "    #output = sys.argv[2]\n",
    "    result_df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format('./../vectors_generation/wiki_vectors_format_with_comments_without_stopwords_2.bin', binary=False)\n",
    "wv.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
      "  app.launch_new_instance()\n",
      "/home/allan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n"
     ]
    }
   ],
   "source": [
    "pattern = \"(?u)\\\\b[\\\\w-]+\\\\b\"\n",
    "\n",
    "fp = np.memmap(\"embed.dat\", dtype=np.double, mode='w+', shape=wv.syn0norm.shape)\n",
    "fp[:] = wv.syn0norm[:]\n",
    "with open(\"embed.vocab\", \"w\") as f:\n",
    "    for _, w in sorted((voc.index, word) for word, voc in wv.vocab.items()):\n",
    "        print(w.encode('utf-8'), file=f)\n",
    "\n",
    "vocab_len = len(wv.vocab)\n",
    "del fp, wv\n",
    "\n",
    "W = np.memmap(\"embed.dat\", dtype=np.double, mode=\"r\", shape=(vocab_len, 300))\n",
    "\n",
    "with open(\"embed.vocab\") as f:\n",
    "    vocab_list = map(str.strip, f.readlines())\n",
    "vocab_dict={w:k for k, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import information of texts to rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_raw = \"./../raw_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_info_text = pd.read_csv(path_raw + raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22279, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_info_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import texts to rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_processed_text = \"./../process_text_expressions/\"\n",
    "processed_text = pd.read_csv(path_processed_text + path_processed_text_data, names = [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [x.strip() for x in processed_text.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.089770636025\n",
      "0.17954127205\n",
      "0.269311908075\n",
      "0.3590825441\n",
      "0.448853180125\n",
      "0.53862381615\n",
      "0.628394452175\n",
      "0.7181650882\n",
      "0.807935724225\n",
      "0.89770636025\n",
      "0.987476996275\n"
     ]
    }
   ],
   "source": [
    "lexicons_rates = list()\n",
    "iter_count = initial_count - 1.\n",
    "for text in texts[initial_count:]:\n",
    "    iter_count += 1.\n",
    "    if(iter_count%2000==0):\n",
    "        print(float(iter_count/len(texts)))\n",
    "        write_csv(lexicons_rates, all_info_text[initial_count:(int(iter_count-1))])\n",
    "    rates = wmd_ratings(text)\n",
    "    lexicons_rates.append(rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_csv(lexicons_rates, all_info_text[initial_count:])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
